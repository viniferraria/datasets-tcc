{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-6d1c3d37-437d-467f-91c8-ef5e89f91db7",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Inicializando tokenizer e stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pillow\n",
    "!pip3 install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00001-f1746c5d-7fd1-480a-87c4-38e9090d575d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1633029369324,
    "source_hash": "5639b14b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wordcloud as wc\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from typing import Union, List\n",
    "from collections import Counter\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.xkcd();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-e2348381-5f52-4df7-8327-ae64cc9115e0",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Criando modelo e carregando stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00001-41e2fd1a-e180-4a70-aa0c-687f4f82bc71",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1633027851973,
    "source_hash": "fbfa8bbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words: np.ndarray = np.array(['because', '’m', 'name', 'therein', '’ll', 'already', 'that',\n",
    "       'hundred', 'her', 'cannot', 'before', 'ever', 'regarding', 'get',\n",
    "       'these', 'as', 'if', 'when', 'onto', 'ours', 'everything', '‘re',\n",
    "       'from', 'whereby', 'side', 'and', 'do', 'must', 'three',\n",
    "       'throughout', 'rather', 'its', 'was', 'amount', 'whose', 'how',\n",
    "       'hereby', 'top', 'see', 'quite', 'thus', 'further', 'last',\n",
    "       'myself', 'enough', 'himself', 'formerly', 'herself', 'more',\n",
    "       'whereafter', 'per', 'yourselves', 'us', 'various', 'everywhere',\n",
    "       'five', 'next', 'below', 'she', 'through', 'once', 'eight',\n",
    "       'which', 'most', 'be', 'above', 'whither', 'wherein', 'up',\n",
    "       'fifty', 'back', 'in', 'seeming', '’ve', 'after', 'full', 'mine',\n",
    "       'yours', 'here', 'out', 'those', 'n‘t', 'eleven', 'all', 'same',\n",
    "       'is', 'however', 'became', 'not', 'either', 'within', 'a', 'part',\n",
    "       'nobody', 'did', 'without', 'many', 'but', 'might', 'nine', 'nor',\n",
    "       'twenty', 'whatever', '’s', 'go', 'former', 'no', 'so', \"'ll\",\n",
    "       'beside', 'therefore', 'about', 'hers', '‘s', 'third', 'much',\n",
    "       \"n't\", 'everyone', 'own', 'over', '‘ve', \"'ve\", 'any', 'other',\n",
    "       'during', 'else', 'still', 'towards', 'bottom', 'his', 'together',\n",
    "       'perhaps', 'though', 'whole', 'besides', 'yourself', 'who',\n",
    "       'using', 'noone', 'made', 'been', 'alone', 'whom', 'around',\n",
    "       'please', 'along', 'are', 'thereupon', 'such', 'latterly', 'very',\n",
    "       'sixty', 'anywhere', 'an', 'am', 'mostly', 'since', 'were',\n",
    "       'become', 'first', 'less', 'moreover', '‘d', 'even', 'does', '’d',\n",
    "       'each', 'now', 'while', 'indeed', 'our', 'becoming', 'empty',\n",
    "       'some', 'unless', 'their', 'both', 'give', 'your', 'anything',\n",
    "       'whereupon', 'nothing', 'of', 'neither', 'upon', 'beyond', 'least',\n",
    "       'say', 'would', '‘ll', 'just', 'every', 'hereupon', 'via', 'down',\n",
    "       'me', 'although', 'into', 'almost', 'seems', 'my', 'becomes',\n",
    "       'whereas', 'latter', 'seem', 'then', 'he', 'serious', 'for',\n",
    "       'front', 'the', 'can', 'few', 're', 'you', 'by', 'could', '’re',\n",
    "       'to', 'six', 'elsewhere', 'than', 'well', \"'d\", 'namely', 'under',\n",
    "       'i', 'someone', 'until', 'anyhow', 'move', 'itself', 'whether',\n",
    "       'put', 'hence', 'toward', 'never', 'often', 'thru', 'or', 'with',\n",
    "       'meanwhile', 'on', 'off', 'at', 'twelve', 'seemed', 'four', 'used',\n",
    "       'done', 'two', 'otherwise', 'beforehand', 'hereafter', 'amongst',\n",
    "       'across', 'between', 'due', 'they', 'call', 'may', 'afterwards',\n",
    "       '‘m', \"'s\", 'one', 'wherever', 'we', 'always', 'has', 'against',\n",
    "       'doing', 'being', 'n’t', 'another', 'should', 'ca', 'except',\n",
    "       'thereby', 'what', 'him', 'forty', 'keep', 'show', 'themselves',\n",
    "       'sometimes', 'whence', 'anyone', 'fifteen', 'it', 'somewhere',\n",
    "       'also', 'take', 'nowhere', 'this', 'nevertheless', 'anyway',\n",
    "       'ourselves', 'will', 'something', 'have', 'there', 'thence', 'why',\n",
    "       \"'re\", 'ten', 'too', 'thereafter', 'none', 'make', 'somehow',\n",
    "       'only', 'others', \"'m\", 'whoever', 'several', 'sometime', 'among',\n",
    "       'had', 'behind', 'whenever', 'yet', 'them', 'really', 'again',\n",
    "       'where', 'herein'], dtype='<U12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-c52f9c44-7349-49d1-b1ef-7c797c87be6f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00008-deb7b50b-8ec1-47d8-bfc3-3727fa5d2c2e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 33,
    "execution_start": 1633027851982,
    "source_hash": "5fdd727",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Prediction\n",
       "0  A very, very, very slow-moving, aimless movie ...           0\n",
       "1  Not sure who was more lost - the flat characte...           0\n",
       "2  Attempting artiness with black & white and cle...           0\n",
       "3       Very little music or anything to speak of.             0\n",
       "4  The best scene in the movie was when Gerardo i...           1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset_filepath: str = os.path.abspath(\"imdb.csv\")\n",
    "imdb_dataset: pd.DataFrame = pd.read_csv(imdb_dataset_filepath, names=[\"Text\", \"Prediction\"], sep=\"\\t\")\n",
    "imdb_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-a8b6ecae-1e3d-4b0b-ac26-798d606ff2f0",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Criação do BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "00015-fddcc20d-bb8c-49b5-ab19-b54988b12e1f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1633033980849,
    "source_hash": "d7846621",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SIA:\n",
    "    __slots__ = [\n",
    "        \"bow\",\n",
    "        \"dbow\",\n",
    "        \"dbow_0\",\n",
    "        \"dbow_1\",\n",
    "        \"word_count\",\n",
    "        \"sentences\",\n",
    "        \"processed_sentences\",\n",
    "        \"classification\",\n",
    "        \"detectors\",\n",
    "        \"vectorized_format_text\"\n",
    "    ]\n",
    "    bow: np.ndarray\n",
    "    dbow: np.ndarray\n",
    "    dbow_0: np.ndarray\n",
    "    dbow_1: np.ndarray\n",
    "    word_count: Counter\n",
    "    sentences: List[str]\n",
    "    processed_sentences: List[str]\n",
    "    classification: List[int]\n",
    "    detectors: List[np.ndarray]\n",
    "    vectorized_format_text: object\n",
    "\n",
    "    def __init__(self, sentences: Union[List[str], np.ndarray], classification: Union[List[int], np.ndarray]) -> None:\n",
    "        # carrega as sentencas\n",
    "        self.sentences = sentences\n",
    "        # carrega as labels das sentencas\n",
    "        self.classification = classification\n",
    "        # vetoriza a funcao de pre processamento\n",
    "        self.vectorized_format_text = np.vectorize(self.format_text)\n",
    "\n",
    "    def format_text(self, text: str) -> str:\n",
    "        # remove caracteres que não sejam letras\n",
    "        fixed_text: str = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "        # remove espacos excesso de espaços\n",
    "        fixed_text = re.sub(r\"\\s{2,}\", r\" \", fixed_text).casefold()\n",
    "        # remove espacos no inicio e no final\n",
    "        fixed_text = re.sub(r\"^\\s+|\\s+$\", \"\", fixed_text)\n",
    "        # quebra a sentenca em tokens (palavras)\n",
    "        tokens = fixed_text.split()\n",
    "        # remove stop words\n",
    "        words = [token for token in tokens if token not in stop_words]\n",
    "        # retorna a sentenca formatada\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def pre_process(self) -> None:\n",
    "        self.processed_sentences: np.ndarray = self.vectorized_format_text(self.sentences)\n",
    "        # cria a bag of words\n",
    "        self.set_bow()\n",
    "        # cria a matriz de bag of words com o algoritmo dbow\n",
    "        self.set_dbow()\n",
    "\n",
    "    def set_bow(self) -> None:\n",
    "        # separa as sentencas em tokens\n",
    "        all_tokens: np.ndarray = np.array((\"\".join(self.processed_sentences).split()))\n",
    "        # conta a frequencia de cada token\n",
    "        self.word_count = Counter(all_tokens)\n",
    "        # cria a bag of words\n",
    "        self.bow = np.array([*self.word_count.keys()])\n",
    "        print(f\"bow with {self.bow.shape[0]} words\")\n",
    "\n",
    "    def set_dbow(self) -> None:\n",
    "        self.dbow = self.generate_dbow(self.sentences.shape[0], self.processed_sentences)\n",
    "        # dbow com sentencas normais\n",
    "        self.dbow_0 = self.dbow[np.where(self.classification == 0)]\n",
    "        # dbow com sentencas classificadas com crise\n",
    "        self.dbow_1 = self.dbow[np.where(self.classification == 1)]\n",
    "\n",
    "    def generate_dbow(self, rows: int, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        # cria uma matriz de zeros com a quantidade de sentencas e a quantidade de palavras presentes no bow \n",
    "        # dbow conta a frequencia de cada palavra da bag of word em cada sentenca\n",
    "        dbow: np.ndarray = np.zeros((rows, self.bow.shape[0]))\n",
    "        for i in range(rows):\n",
    "            # conta a quantidade de ocorrencias de cada palavra na sentenca\n",
    "            sentence_counter: Counter = Counter(sentences[i].split())\n",
    "            for j in range(dbow.shape[1]):\n",
    "                # tokens que ocorrem pelo menos uma vez na sentenca são considerados\n",
    "                # como 1 na matriz de bag of words\n",
    "                dbow[i][j] = 1 if sentence_counter[self.bow[j]] > 0 else 0\n",
    "        return dbow\n",
    "\n",
    "    def generate_detectors(self, number_of_detectors: int) -> None:\n",
    "        self.detectors = []\n",
    "        # loop para criacao de detectores\n",
    "        while len(self.detectors) < number_of_detectors:\n",
    "            # gera um candidato aleatorio a ser um detector\n",
    "            candidate_detector: np.ndarray = self.generate_candidate_detector()\n",
    "            for d_row in range(self.dbow_0):\n",
    "                # verifica se o candidato a detector dá match com alguma linha do dbow (self)\n",
    "                if self.match(d_row, candidate_detector):\n",
    "                    break\n",
    "            else:\n",
    "                self.detectors.append(candidate_detector)\n",
    "\n",
    "    def generate_candidate_detector(self, word_probability=0.01) -> np.ndarray:\n",
    "        # cria uma matriz de zeros com a quantidade de palavras do bow\n",
    "        detector: np.ndarray = np.zeros(self.bow.shape[0])\n",
    "        for i in range(detector.shape[0]):\n",
    "            # gera um numero aleatorio entre 0 e 1\n",
    "            # se o numero for menor que a probabilidade de ocorrencia da palavra,\n",
    "            # a palavra é considerada como 1 (ocorre na frase)\n",
    "            if np.random.rand() < word_probability:\n",
    "                detector[i] = 1\n",
    "        return detector\n",
    "\n",
    "    def match(self, match_set: np.ndarray, detector: np.ndarray, threshold=5) -> bool:\n",
    "        # verifica se o detector dá match com alguma linha do dbow\n",
    "        # se o numero de ocorrencias de uma palavra for maior que o threshold,\n",
    "        # o detector dá match com a linha do dbow\n",
    "        return ((match_set == detector).astype(int).sum() >= threshold)\n",
    "\n",
    "    def detect(self, sentences: np.ndarray) -> np.ndarray:\n",
    "        # matriz de resultados\n",
    "        classification_results: np.ndarray = np.zeros(sentences.shape[0], np.int8)\n",
    "        # pre processa as sentencas\n",
    "        pre_processed_sentences: np.ndarray = self.vectorized_format_text(sentences)\n",
    "        # cria a dbow com as sentencas pre processadas\n",
    "        detect_dbow: np.ndarray = self.generate_dbow(\n",
    "            pre_processed_sentences.shape[0], pre_processed_sentences)\n",
    "\n",
    "        for i in range(detect_dbow.shape[0]):\n",
    "            for detector in self.detectors:\n",
    "                # verifica se o detector dá match com alguma linha do dbow\n",
    "                if self.match(detect_dbow[i], detector):\n",
    "                    # se o detector dá match, a sentenca é classificada como crise\n",
    "                    classification_results[i] = 1\n",
    "                    break\n",
    "        return classification_results\n",
    "\n",
    "    def export(self, file_path=os.getcwd()) -> None:\n",
    "        with open(os.path.abspath(os.path.join(file_path, \"bow.txt\")), \"w+\") as f:\n",
    "            f.write(str(list(self.bow)))\n",
    "        with open(os.path.abspath(os.path.join(file_path, \"dbow.txt\")), \"w+\") as f:\n",
    "            f.write(str(list(self.dbow)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "00016-a838cc29-9fde-44e0-bcd4-e337b0ae4dc4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1633033981046,
    "source_hash": "5b9ef154",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cria um SIA com a base do imbd\n",
    "s: SIA = SIA(imdb_dataset[\"Text\"].to_numpy(), imdb_dataset[\"Prediction\"].to_numpy())\n",
    "s.pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "00018-a5c35a27-ea95-4474-a69f-9ba7d92b07e4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 26,
    "execution_start": 1633033986356,
    "source_hash": "40deb086",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \n",
      "slowmoving aimless movie distressed drifting young man\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentenca original: {s.sentences[0]}\")\n",
    "print(f\"Sentenca pre processada: {s.processed_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "00012-3dd718a4-4293-44e7-8d9c-40f18d377cb2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1633033986368,
    "source_hash": "c917b164",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 112),\n",
       " ('film', 111),\n",
       " ('bad', 44),\n",
       " ('good', 41),\n",
       " ('like', 40),\n",
       " ('great', 32),\n",
       " ('characters', 29),\n",
       " ('acting', 29),\n",
       " ('movies', 28),\n",
       " ('time', 27)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mostrando as palavras que aparecem mais vezes na base\n",
    "s.word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = dict(s.word_count.most_common(5))\n",
    "plt.bar(list(word_data.keys()), list(word_data.values()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = wc.WordCloud(background_color=\"white\", width=1920, height=1080)\n",
    "word_cloud.generate_from_frequencies(s.word_count)\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-fe4e43e8-ccbc-40e5-984b-eeb1e4623653",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 73,
    "execution_start": 1633033986373,
    "source_hash": "e07a7d7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gerando detectores\n",
    "s.generate_detectors(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00019-6b7ddeb6-28a3-4b0d-ab79-dcc5c2f8dabf",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 19,
    "execution_start": 1633033987210,
    "source_hash": "2b4c44da",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 1.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-beee71c2-4db2-4598-b888-378583980e09",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11924,
    "execution_start": 1633033989251,
    "source_hash": "1d717f81",
    "tags": []
   },
   "outputs": [],
   "source": [
    "s.detect(imdb_dataset.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "00016-7f4b8c22-b11f-4792-a9f4-e2bca2412eb3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4989,
    "execution_start": 1633028650186,
    "source_hash": "11063a96",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_processed_sentences = s.vectorized_format_text(imdb_dataset[\"Text\"])\n",
    "new_dbow = s.generate_dbow(pre_processed_sentences.shape[0], pre_processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "cell_id": "00019-01df1387-ac84-46fd-aa62-5c9fb21662fc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22,
    "execution_start": 1633028591581,
    "source_hash": "bf59cd52",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['slowmoving', 'aimless', 'movie', 'distressed', 'drifting',\n",
       "       'young', 'man'], dtype='<U34')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.bow[np.where(new_dbow[0]>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-5c78e7b6-e14d-4980-b183-49d2ba124760",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=993f98d4-e474-42c5-8e20-241471545034' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9ba24730-b435-428d-8441-14169b6d600b",
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
